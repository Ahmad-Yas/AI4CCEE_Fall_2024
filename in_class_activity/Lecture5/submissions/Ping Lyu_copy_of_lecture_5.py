# -*- coding: utf-8 -*-
"""Copy of lecture 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L2iM0yJTHB7x6V2f590ez2NxI2i7NzNA
"""

import pandas as pd

df = pd.read_csv('/content/Car_Speed.csv')
print(df)

# prompt: Plot the data points on a scatter plot (time vs. speed) for first 1253 rows

import matplotlib.pyplot as plt

# Assuming 'Time' and 'Speed' are the column names in your DataFrame
plt.scatter(df['Time'][:1253], df['Speed'][:1253])
plt.xlabel('Time')
plt.ylabel('Speed')
plt.title('Time vs. Speed (First 1253 Rows)')
plt.show()

# prompt: Split the dataset into training and testing sets

from sklearn.model_selection import train_test_split

# Assuming 'Speed' is your target variable
X = df[['Time']]  # Features (independent variables)
y = df['WAVE-SW-Speed']    # Target (dependent variable)

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the shapes of the resulting datasets to verify the split
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# prompt: Train a simple linear regression model using Python (scikit-learn).

from sklearn.linear_model import LinearRegression

# Create a linear regression model
model = LinearRegression()

# Convert 'Time' column to numerical representation
# (e.g., convert to seconds since a specific starting point)
X_train['Time'] = pd.to_datetime(X_train['Time'])
X_train['Time'] = (X_train['Time'] - X_train['Time'].min()).dt.total_seconds()

# Train the model on the training data
model.fit(X_train, y_train)

# Print the model's coefficients (slope and intercept)
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

# prompt: Train a simple linear regression model using Python (scikit-learn).

from sklearn.linear_model import LinearRegression

# Create a linear regression model
model = LinearRegression()

# Convert 'Time' column to numerical representation
# (e.g., convert to seconds since a specific starting point)
X_train['Time'] = pd.to_datetime(X_train['Time'])
X_train['Time'] = (X_train['Time'] - X_train['Time'].min()).dt.total_seconds()

# Remove rows with NaN values in 'y_train'
X_train = X_train[~y_train.isna()]
y_train = y_train[~y_train.isna()]

# Train the model on the training data
model.fit(X_train, y_train)

# Print the model's coefficients (slope and intercept)
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

# prompt: lot the data points on a scatter plot (time vs. speed) for first 24 hours

import matplotlib.pyplot as plt

# Assuming 'Time' and 'Speed' are the column names in your DataFrame
plt.scatter(df['Time'][:24*60*60], df['WAVE-NE-Speed'][:24*60*60])
plt.xlabel('Time')
plt.ylabel('Speed')
plt.title('Time vs. Speed (First 24 Hours)')
plt.show()

# prompt: Train a simple linear regression model using Python (scikit-learn) and plot it

# Assuming 'Time' and 'Speed' are the column names in your DataFrame
X = df[['Time']]  # Features (independent variables)
y = df['WAVE-NE-Speed']    # Target (dependent variable)

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Create a linear regression model
model = LinearRegression()

# Convert 'Time' column to numerical representation
# (e.g., convert to seconds since a specific starting point)
X_train['Time'] = pd.to_datetime(X_train['Time'])
X_train['Time'] = (X_train['Time'] - X_train['Time'].min()).dt.total_seconds()

# Remove rows with NaN values in 'y_train'
X_train = X_train[~y_train.isna()]
y_train = y_train[~y_train.isna()]

# Train the model on the training data
model.fit(X_train, y_train)

# Print the model's coefficients (slope and intercept)
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)


# Make predictions on the training data
y_pred = model.predict(X_train)

# Plot the regression line
plt.scatter(X_train, y_train, color='blue')
plt.plot(X_train, y_pred, color='red', linewidth=2)
plt.xlabel('Time')
plt.ylabel('WAVE-NE-Speed')
plt.title('Linear Regression: Time vs. WAVE-NE-Speed')
plt.show()

# prompt: Train a simple linear regression model using Python (scikit-learn)

# Assuming 'Time' and 'Speed' are the column names in your DataFrame
X = df[['Time']]  # Features (independent variables)
y = df['WAVE-NE-Count']    # Target (dependent variable)

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Create a linear regression model
model = LinearRegression()

# Convert 'Time' column to numerical representation
# (e.g., convert to seconds since a specific starting point)
X_train['Time'] = pd.to_datetime(X_train['Time'])
X_train['Time'] = (X_train['Time'] - X_train['Time'].min()).dt.total_seconds()

# Remove rows with NaN values in 'y_train'
X_train = X_train[~y_train.isna()]
y_train = y_train[~y_train.isna()]

# Train the model on the training data
model.fit(X_train, y_train)

# Print the model's coefficients (slope and intercept)
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

# Make predictions on the testing data
X_test['Time'] = pd.to_datetime(X_test['Time'])
X_test['Time']

# prompt: Use the testing data to evaluate the modelâ€™s performance

# X_test['Time'] = (X_test['Time'] - X_test['Time'].min()).dt.total_seconds() # This line is removed as the conversion has already been done
y_pred = model.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Calculate R-squared (R2)
r2 = r2_score(y_test, y_pred)
print("R-squared:", r2)

# Make predictions on the testing data
X_test['Time'] = pd.to_datetime(X_test['Time'])
X_test['Time'] = (X_test['Time'] - X_test['Time'].min()).dt.total_seconds()

# Remove rows with NaN values in 'y_test'
X_test = X_test[~y_test.isna()]
y_test = y_test[~y_test.isna()]

y_pred = model.predict(X_test)

# Evaluate the model's performance (e.g., using R-squared)
r2 = r2_score(y_test, y_pred)
print("R-squared:", r2)

# You can also calculate other metrics like Mean Squared Error (MSE) or Mean Absolute Error (MAE)
from sklearn.metrics import mean_squared_error, mean_absolute_error

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)

# prompt: Calculate metrics like Mean Absolute Error (MAE) or Root Mean Squared Error
# (RMSE)

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Assuming you have y_test and y_pred from your model's predictions

# Calculate MAE
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error:", rmse)

# prompt: Split the dataset into training and testing sets

# Assuming 'Time' and 'Speed' are the column names in your DataFrame
X = df[['Time']]  # Features (independent variables)
y = df['WAVE-NE-Count']    # Target (dependent variable)

# Split the data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the shapes of the resulting datasets to verify the split
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# prompt: Use KMeans clustering to group data points into clusters.

from sklearn.cluster import KMeans
import pandas as pd

# Assuming you have your data in a DataFrame called 'df' and want to cluster based on 'Time' and 'WAVE-NE-Count'
X = df[['Time', 'WAVE-NE-Speed']].dropna()  # Remove rows with NaN values

# Convert 'Time' column to numeric representation (seconds since epoch)
X['Time'] = pd.to_datetime(X['Time'])
X['Time'] = (X['Time'] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1s')

# Choose the number of clusters (you can experiment with different values)
n_clusters = 3

# Create a KMeans model
kmeans = KMeans(n_clusters=n_clusters, random_state=42)

# Fit the model to your data
kmeans.fit(X)

# Get the cluster labels for each data point
cluster_labels = kmeans.labels_

# Add the cluster labels to your DataFrame
X['Cluster'] = cluster_labels

# You can now analyze the clusters, for example, by plotting them:
# plt.scatter(X['Time'], X['WAVE-NE-Count'], c=X['Cluster']) # This line will cause an error if WAVE-NE-Count column is not present in the DataFrame
plt.scatter(X['Time'], X['WAVE-NE-Speed'], c=X['Cluster'])
plt.xlabel('Time')
plt.ylabel('WAVE-NE-Speed')
plt.title('KMeans Clustering')
plt.show()

# You can also analyze the characteristics of each cluster
# by looking at the mean values of your features within each cluster.
print(X.groupby('Cluster').mean())

# prompt: Visualize the clusters on the scatter plot.

# Assuming you have your data in a DataFrame called 'X' with 'Time', 'WAVE-NE-Speed', and 'Cluster' columns

# Visualize the clusters on the scatter plot
plt.figure(figsize=(10, 6))  # Adjust figure size as needed
plt.scatter(X['Time'], X['WAVE-NE-Speed'], c=X['Cluster'], cmap='viridis')
plt.xlabel('Time')
plt.ylabel('WAVE-NE-Speed')
plt.title('KMeans Clustering of WAVE-NE-Speed vs. Time')
plt.colorbar(label='Cluster')  # Add a colorbar to indicate cluster labels
plt.show()

# prompt: Discuss the meaning of each cluster

# Assuming you have your data in a DataFrame called 'X' with 'Time', 'WAVE-NE-Speed', and 'Cluster' columns

# Analyze the characteristics of each cluster
cluster_analysis = X.groupby('Cluster').agg({'Time': ['mean', 'std'], 'WAVE-NE-Speed': ['mean', 'std']})
print(cluster_analysis)


# You can then interpret the clusters based on these characteristics.
# For example:

# * Cluster 0: Might represent periods with generally low WAVE-NE-Speed and a certain average time.
# * Cluster 1: Might indicate periods with moderate WAVE-NE-Speed and a different average time compared to cluster 0.
# * Cluster 2: Might correspond to periods with high WAVE-NE-Speed and potentially a different average time.

# Interpretation Example:
print("Possible Cluster Interpretations:")

for cluster_id in cluster_analysis.index:
    mean_speed = cluster_analysis.loc[cluster_id, ('WAVE-NE-Speed', 'mean')]
    mean_time = cluster_analysis.loc[cluster_id, ('Time', 'mean')]

    if mean_speed < 50:
        print(f"Cluster {cluster_id}: Low WAVE-NE-Speed, average Time: {mean_time}")
    elif 50 <= mean_speed < 100:
        print(f"Cluster {cluster_id}: Moderate WAVE-NE-Speed, average Time: {mean_time}")
    else:
        print(f"Cluster {cluster_id}: High WAVE-NE-Speed, average Time: {mean_time}")

# You can add more analysis (e.g., histograms, box plots) to further understand the clusters.

# prompt: Explore other clustering algorithms (e.g., DBSCAN, hierarchical clustering).

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch

# Assuming you have your data in a DataFrame called 'X' with 'Time' and 'WAVE-NE-Speed' columns

# Preprocess the data by scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X[['Time', 'WAVE-NE-Speed']])

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)  # You can experiment with these parameters
dbscan_labels = dbscan.fit_predict(X_scaled)

X['DBSCAN_Cluster'] = dbscan_labels

# Visualize DBSCAN clusters
plt.figure(figsize=(10, 6))
plt.scatter(X['Time'], X['WAVE-NE-Speed'], c=X['DBSCAN_Cluster'], cmap='viridis')
plt.xlabel('Time')
plt.ylabel('WAVE-NE-Speed')
plt.title('DBSCAN Clustering of WAVE-NE-Speed vs. Time')
plt.colorbar(label='Cluster')
plt.show()

# Hierarchical Clustering
agg_clustering = AgglomerativeClustering(n_clusters=3)  # You can experiment with n_clusters
agg_labels = agg_clustering.fit_predict(X_scaled)

X['Hierarchical_Cluster'] = agg_labels

# Visualize Hierarchical clusters
plt.figure(figsize=(10, 6))
plt.scatter(X['Time'], X['WAVE-NE-Speed'], c=X['Hierarchical_Cluster'], cmap='viridis')
plt.xlabel('Time')
plt.ylabel('WAVE-NE-Speed')
plt.title('Hierarchical Clustering of WAVE-NE-Speed vs. Time')
plt.colorbar(label='Cluster')
plt.show()

# Dendrogram for Hierarchical clustering
plt.figure(figsize=(12, 6))
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Data points')
plt.ylabel('Euclidean distances')
plt.show()

# prompt: Explore other clustering algorithms (e.g., DBSCAN, hierarchical clustering).

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch
import pandas as pd # import pandas

# Assuming you have your data in a DataFrame called 'df' with 'Time' and 'WAVE-NE-Speed' columns
# Create a sample DataFrame (replace this with your actual data)
data = {'Time': [1678886400, 1678886460, 1678886520, 1678886580, 1678886640],
        'WAVE-NE-Speed': [55, 60, 62, 58, 53]}
df = pd.DataFrame(data)

# Convert 'Time' column to numeric representation (seconds since epoch)
df['Time'] = pd.to_datetime(df['Time'])
df['Time'] = (df['Time'] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1s')

# Preprocess the data by scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[['Time', 'WAVE-NE-Speed']]) # Use df instead of X

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)  # You can experiment with these parameters
dbscan_labels = dbscan.fit_predict(X_scaled)

df['DBSCAN_Cluster'] = dbscan_labels # Use df instead of X

# Visualize DBSCAN clusters
plt.figure(figsize=(10, 6))
plt.scatter(df['Time'], df['WAVE-NE-Speed'], c=df['DBSCAN_Cluster'], cmap='viridis') # Use df instead of X
plt.xlabel('Time')
plt.ylabel('WAVE-NE-Speed')
plt.title('DBSCAN Clustering of WAVE-NE-Speed vs. Time')
plt.colorbar(label='Cluster')
plt.show()

# Hierarchical Clustering
agg_clustering = AgglomerativeClustering(n_clusters=3)  # You can experiment with n_clusters
agg_labels = agg_clustering.fit_predict(X_scaled)

df['Hierarchical_Cluster'] = agg_labels # Use df instead of X

# Visualize Hierarchical clusters
plt.figure(figsize=(10, 6))
plt.scatter(df['Time'], df['WAVE-NE-Speed'], c=df['Hierarchical_Cluster'], cmap='viridis') # Use df instead of X
plt.xlabel('Time')
plt.ylabel('WAVE-NE-Speed')
plt.title('Hierarchical Clustering of WAVE-NE-Speed vs. Time')
plt.colorbar(label='Cluster')
plt.show()

# Dendrogram for Hierarchical clustering
plt.figure(figsize=(12, 6))
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Data points')
plt.ylabel('Euclidean distances')
plt.show()

# prompt: Explore other clustering algorithms (e.g., DBSCAN, hierarchical clustering).

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch
import pandas as pd # import pandas
import matplotlib.pyplot as plt # import matplotlib.pyplot

# Assuming you have your data in a DataFrame called 'df' with 'Time' and 'WAVE-NE-Speed' columns
# Create a sample DataFrame (replace this with your actual data)
data = {'Time': [1678886400, 1678886460, 1678886520, 1678886580, 1678886640],
        'WAVE-NE-Speed': [55, 60, 62, 58, 53]}
df = pd.DataFrame(data)

# Convert 'Time' column to numeric representation (seconds since epoch)
df['Time'] = pd.to_datetime(df['Time'])
df['Time'] = (df['Time'] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1s')

# Preprocess the data by scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[['Time', 'WAVE-NE-Speed']]) # Use df instead of X

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)  # You can experiment with these parameters
dbscan_labels = dbscan.fit_predict(X_scaled)

df['DBSCAN_Cluster'] = dbscan_labels # Use df instead of X

# Visualize DBSCAN clusters
plt.figure(figsize=(10, 6))
plt.scatter(df['Time'], df['WAVE-NE-Speed'], c=df['DBSCAN_Cluster'], cmap='viridis') # Use df instead of X
plt.xlabel('Time')
plt.ylabel('WAVE-NE-Speed')
plt.title('DBSCAN Clustering of WAVE-NE-Speed vs. Time')
plt.colorbar(label='Cluster')
plt.show()

# Hierarchical Clustering
agg_clustering = AgglomerativeClustering(n_clusters=3)  # You can experiment with n_clusters
agg_labels = agg_clustering.fit_predict(X_scaled)

df['Hierarchical_Cluster'] = agg_labels # Use df instead of X

# Visualize Hierarchical clusters
plt.figure(figsize=(10, 6))
plt.scatter(df['Time'], df['WAVE-NE-Speed'], c=df['Hierarchical_Cluster'], cmap='viridis') # Use df instead of X
plt.xlabel('Time')
plt.ylabel('WAVE-NE-Speed')
plt.title('Hierarchical Clustering of WAVE-NE-Speed vs. Time')
plt.colorbar(label='Cluster')
plt.show()

# Dendrogram for Hierarchical clustering
plt.figure(figsize=(12, 6))
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Data points')
plt.ylabel('Euclidean distances')
plt.show()

# prompt: In what scenarios would each clustering technique be most appropriate?

# In what scenarios would each clustering technique be most appropriate?

# KMeans:
# - Suitable for:
#   - Finding clusters with relatively equal sizes and spherical shapes.
#   - When the number of clusters is known or can be estimated beforehand.
#   - Situations where speed and efficiency are important (relatively faster than hierarchical and DBSCAN).
# - Example scenarios:
#   - Customer segmentation based on purchasing behavior.
#   - Image segmentation based on color or texture features.
#   - Document clustering based on word frequency.

# DBSCAN:
# - Suitable for:
#   - Discovering clusters of arbitrary shapes and sizes.
#   - Identifying outliers or noise in the data.
#   - When the number of clusters is not known in advance.
# - Example scenarios:
#   - Detecting anomalies in network traffic or sensor data.
#   - Grouping geographical locations based on density.
#   - Finding clusters in spatial data, where clusters may not be uniformly distributed.

# Hierarchical Clustering:
# - Suitable for:
#   - Understanding the hierarchical relationships between data points.
#   - When you want to explore the structure of the data at multiple levels of granularity.
#   - Generating a dendrogram to visualize the clustering process.
# - Example scenarios:
#   - Gene expression analysis (clustering genes with similar expression patterns).
#   - Social network analysis (identifying communities or groups).
#   - Taxonomy and classification tasks (grouping organisms based on evolutionary relationships).

# In summary:
# - If you know the number of clusters and need a fast and efficient algorithm, KMeans is a good choice.
# - If you want to discover clusters of various shapes and sizes, including outlier detection, consider DBSCAN.
# - If you need to understand the hierarchy and relationships between data points and visualize the clustering process, hierarchical clustering is more appropriate.

# Note: Choosing the best technique depends on the specific data, the desired outcome, and the computational resources available.
# It's often beneficial to experiment with multiple clustering techniques and compare their results to see which one provides the most meaningful and useful clustering for your application.

!pip install xlrd
import pandas as pd

df = pd.read_excel('/content/Ames_August_Crashes.xls') # Use read_excel for .xls files
print(df)

# prompt: Display the first few rows of the dataset using df.head()

print(df.head())

# prompt: Check for missing values and data types using df.info()

df.info()

# prompt: Create summary statistics using df.describe()

summary_stats = df.describe()
print(summary_stats)

# prompt: Identify the target variable = Severity

# Assuming 'Severity' is the target variable
target_variable = 'Severity'

# prompt: Select relevant features for prediction : Time of day, driver age, vehicle type

# Select relevant features
features = ['Time', 'Driver Age', 'Vehicle Type']

# Create a new DataFrame with only the selected features and target variable
df_selected = df[features + [target_variable]].copy()

# Print the first few rows of the selected DataFrame
print(df_selected.head())

df = pd.read_csv('/content/Ames_August_Crashes.csv')
print(df)

# prompt: Identify the target variable = Severity

# Assuming 'Severity' is the target variable
target_variable = 'Severity'

import pandas as pd

# Define the path to your CSV file
input_file_path = 'path_to_your_file.csv'
output_file_path = 'cleaned_crash_report.csv'

# Read the CSV file while skipping unnecessary rows
df = pd.read_csv('/content/Ames_August_Crashes.csv', skiprows=6)

# Clean up any unwanted columns (if needed)
# Here, we drop the empty columns that resulted from the original formatting
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

# Optionally, reset the index if needed
df.reset_index(drop=True, inplace=True)

# Save the cleaned DataFrame to a new CSV file
df.to_csv(output_file_path, index=False)

print(f"CSV file has been cleaned and saved as '{output_file_path}'.")

# prompt: Display the first few rows of the dataset using df.head().
# Check for missing values and data types using df.info().
# Create summary statistics using df.describe().

# Display the first few rows of the dataset
print(df.head())

# Check for missing values and data types
df.info()

# Create summary statistics
summary_stats = df.describe()
print(summary_stats)

# prompt: Identify the target variable = crash severity

target_variable = 'Severity'